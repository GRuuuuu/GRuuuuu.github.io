---
title: "호다닥 공부해보는 RNN 친구들(Recurrent Neural Networks)"
categories: 
  - Machine-Learning
tags:
  - ML
  - RNN
  - LSTM
  - GRU
last_modified_at: 2019-09-23T13:00:00+09:00
author_profile: true
sitemap :
  changefreq : daily
  priority : 1.0
---

## Overview
호다닥 공부해보는 시리즈가 2편째가 되었습니다. 이번에는 머신러닝의 꽃이라고도 불리는 `RNN`을 들고 왔습니다. `RNN`친구들인 `LSTM`과 `GRU`도 소개하도록 하겠습니다.  

# RNN?
## Concept
`RNN`은 그 이름에서 알 수 있듯이, **Recurrent**한 모델입니다.   
다시말하면, input->`RNN`->output->input->`RNN`->output....이러한 구조가 계속 반복되는 모델입니다.  
생긴게 1자로 쭉 이어져서 **sequence**한 모델이라고도 합니다.  
아래 사진은 위에서 언급한 RNN의 성질을 표현한 그림입니다.  

![1](https://user-images.githubusercontent.com/15958325/65409665-3f602d00-de23-11e9-8185-01c7082ec6de.png)  

![image](https://user-images.githubusercontent.com/15958325/65409847-c1505600-de23-11e9-9389-60843b383748.png)  

A의 꼬부랑선은 단순히 아래 그림처럼 반복된다는 것을 표현한 선입니다.  

과거 정보를 활용하는 순환적인 구조를 띄고 있기에 RNN은 연속적인 이벤트, 또는 자연어 처리 용도로 사용되어 왔습니다.    

<이벤트 사진>

<자연어 처리 사진> 

## Architecture
그럼 지금부터는 수식을 통해 좀 더 자세히 알아보겠습니다.  
