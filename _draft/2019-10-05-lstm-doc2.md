---
title: "호다닥 공부해보는 RNN 친구들(2) - LSTM & GRU"
categories: 
  - Machine-Learning
tags:
  - ML
  - RNN
  - LSTM
  - GRU
last_modified_at: 2019-10-05T13:00:00+09:00
author_profile: true
sitemap :
  changefreq : daily
  priority : 1.0
---
[호다닥 공부해보는 RNN 친구들(1) - RNN(Recurrent Neural Networks)](https://gruuuuu.github.io/machine-learning/lstm-doc/)에서 이어지는 글입니다.  

## Vanishing Gradient Problem
기존의 RNN은 출력결과를 내기위해 이전의 계산 결과에 의존합니다. 과거의 정보를 전부 포함하여 계산한다는 장점도 있지만, 시간(time step)이 흐를수록 과거의 정보가 옅어진다는 단점도 있습니다.  

<center><img src="https://user-images.githubusercontent.com/15958325/66251780-be902200-e78e-11e9-92f8-7cfa72153978.png"></center>  
<center>처음 정보가 시간이 흐를수록 점점 옅어짐</center>  

위 그림처럼 처음 Hello라는 단어를 계속 기억하고 있다가 정보가 점점 흐려져 마지막에는 엉뚱한 단어를 결과로 낼 수 있습니다.  

처음 입력으로 받은 정보는 학습에 강한 영향을 끼치다가 점점 새로운 입력이 들어오면서 처음 입력값의 영향력은 점차 감소하고 결국 학습에 아무런 영향을 끼치지 못하게 됩니다.  


![image](https://user-images.githubusercontent.com/15958325/66256055-1c3f6100-e7c5-11e9-907b-c6182256a32c.png)


이를 RNN의 **장기 의존성 문제**(the problem of Long-Term Dependencies)라고도 하고, 네트워크 학습에 사용하는 경사도(Gradient)가 사라지는(Vanishing)현상이므로 **Vanishing Gradient Problem**이라고도 부릅니다.  






# LSTM?
## Concept

