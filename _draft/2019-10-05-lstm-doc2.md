---
title: "호다닥 공부해보는 RNN 친구들(2) - LSTM & GRU"
categories: 
  - Machine-Learning
tags:
  - ML
  - RNN
  - LSTM
  - GRU
last_modified_at: 2019-10-05T13:00:00+09:00
author_profile: true
sitemap :
  changefreq : daily
  priority : 1.0
---
[호다닥 공부해보는 RNN 친구들(1) - RNN(Recurrent Neural Networks)](https://gruuuuu.github.io/machine-learning/lstm-doc/)에서 이어지는 글입니다.  

## RNN의 고질적인 문제
이전의 계산결과를 바탕으로 계산한다는 RNN의 특징은 어찌보면 사람의 뇌와 많이 닮았습니다. 그래서 Neural Network라는 이름을 붙인것일지도 모릅니다.  
그래서 사람의 신경망과 닮은 이 특징은 단점도 가지고 있습니다.  

>여러분은 **올해의 신년 다짐**이 뭐였는지 기억나시나요?  
>작년은? 10년전은 기억하고 계신가요?  

물론 어딘가에 기록해두고 꾸준히 실천하고 계신 분들도 있겠지만, 대다수는 그 당시에만 반짝 실천하고 시간이 흐를수록 잊어버리겠죠. 저는 기억도 안납니다.  

RNN도 비슷합니다.  
처음 입력으로 받은 정보는 학습에 강한 영향을 끼치다가 점점 새로운 입력이 들어오면서 처음 입력값의 영향력은 점차 감소하고 결국 학습에 아무런 영향을 끼치지 못하게 됩니다.  

![image](https://user-images.githubusercontent.com/15958325/66256055-1c3f6100-e7c5-11e9-907b-c6182256a32c.png)


이를 RNN의 **장기 의존성 문제**(the problem of Long-Term Dependencies)라고도 하고, 네트워크 학습에 사용하는 경사도(Gradient)가 사라지는(Vanishing)현상이므로 **Vanishing Gradient Problem**이라고도 부릅니다.  

## Vanishing(or Exploding) Gradient Problem
이 문제는 오랜시간 Deep Learning 영역에서 골치를 썩여왔었습니다.   
지금부터는 왜 이런 현상이 발생하는지 알아보도록 하겠습니다.  

먼저 신경망의 역전파에 대해서 알고있어야 하는데 자세한 설명은 다음 동영상으로 대체하고 진행하겠습니다.  :  
[lec9-2: 딥넷트웍 학습 시키기 (backpropagation)](https://youtu.be/573EZkzfnZ0)   

먼저 역전파를 하는 이유는 **결과의 오류를 바로잡기 위함**이고, 이를 위해 각 layer들의 가중치(w)를 조절하게 됩니다.  

cost함수를 최적화하기 위해 기울기를 구하게 되고 이때 기울기는 해당 함수의 미적분값이겠죠.  

문제는 여기서 발생합니다.  
신경망이 **곱하기연산**을 기반으로 만들어져 있기 때문에, 미적값이 1보다 조금만 커도 gradient값이 **발산**하게 되고 1보다 조금만 작아도 gradient값이 **소실**하게 됩니다.  

> EX)  
> 1.1^100 = 13,780.6  
> 0.9^100 = 0.0000265



# LSTM?
## Concept

