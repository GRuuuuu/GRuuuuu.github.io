---
title: "호다닥 공부해보는 RNN 친구들(2) - LSTM & GRU"
categories: 
  - Machine-Learning
tags:
  - ML
  - RNN
  - LSTM
  - GRU
last_modified_at: 2019-10-05T13:00:00+09:00
author_profile: true
sitemap :
  changefreq : daily
  priority : 1.0
---
[호다닥 공부해보는 RNN 친구들(1) - RNN(Recurrent Neural Networks)](https://gruuuuu.github.io/machine-learning/lstm-doc/)에서 이어지는 글입니다.  

## RNN의 고질적인 문제
이전의 계산결과를 바탕으로 계산한다는 RNN의 특징은 어찌보면 사람의 뇌와 많이 닮았습니다. 그래서 Neural Network라는 이름을 붙인것일지도 모릅니다.  
그래서 사람의 신경망과 닮은 이 특징은 단점도 가지고 있습니다.  

>여러분은 **올해의 신년 다짐**이 뭐였는지 기억나시나요?  
>작년은? 10년전은 기억하고 계신가요?  

물론 어딘가에 기록해두고 꾸준히 실천하고 계신 분들도 있겠지만, 대다수는 그 당시에만 반짝 실천하고 시간이 흐를수록 잊어버리겠죠. 저는 기억도 안납니다.  

RNN도 비슷합니다.  
처음 입력으로 받은 정보는 학습에 강한 영향을 끼치다가 점점 새로운 입력이 들어오면서 처음 입력값의 영향력은 점차 감소하고 결국 학습에 아무런 영향을 끼치지 못하게 됩니다.  

![image](https://user-images.githubusercontent.com/15958325/66256055-1c3f6100-e7c5-11e9-907b-c6182256a32c.png)


이를 RNN의 **장기 의존성 문제**(the problem of Long-Term Dependencies)라고도 하고, 네트워크 학습에 사용하는 경사도(Gradient)가 사라지는(Vanishing)현상이므로 **Vanishing Gradient Problem**이라고도 부릅니다.  

## Vanishing(or Exploding) Gradient Problem
이 문제는 오랜시간 Deep Learning 영역에서 골치를 썩여왔었습니다.   
지금부터는 왜 이런 현상이 발생하는지 알아보도록 하겠습니다.  

먼저 신경망의 역전파에 대해서 알고있어야 하는데 자세한 설명은 다음 동영상으로 대체하고 진행하겠습니다.  :  
[lec9-2: 딥넷트웍 학습 시키기 (backpropagation)](https://youtu.be/573EZkzfnZ0)   

먼저 역전파를 하는 이유는 **결과의 오류를 바로잡기 위함**이고, 이를 위해 각 layer들의 가중치(w)를 조절하게 됩니다.  

cost함수를 최적화하기 위해 기울기를 구하게 되고 이때 기울기는 해당 함수의 미적분값이겠죠.  

문제는 여기서 발생합니다.  
신경망이 **곱하기연산**을 기반으로 만들어져 있기 때문에, 미적값이 1보다 조금만 커도 gradient값이 **발산**하게 되고 1보다 조금만 작아도 gradient값이 **소실**하게 됩니다.  

> EX)  
> 1.1^100 = 13,780.6  
> 0.9^100 = 0.0000265

아래 사진은 sigmoid와 tanh의 그래프와 미적 그래프 입니다.   
![image](https://user-images.githubusercontent.com/15958325/66532733-7042a080-eb4b-11e9-9c0a-23cfd82474fb.png)   
![image](https://user-images.githubusercontent.com/15958325/66532761-910af600-eb4b-11e9-8c67-203f36f022cc.png)   

sigmoid와 tanh 모두 미분함수가 |x|가 커질수록 0에 수렴하는 모양을 보이며 Backpropagation시 미분값이 소실될 가능성이 큽니다.  

그래서 기존 RNN의 **장기 의존성 문제**(the problem of Long-Term Dependencies)가 발생하게 되었고, 이 다음부터 LSTM이 어떻게 문제를 해결하였는지에 대해 설명하도록 하겠습니다.  

> **[리빙포인트 -ReLU]**   
>
> ![image](https://user-images.githubusercontent.com/15958325/66532803-b13ab500-eb4b-11e9-99e2-e01c372b5d93.png)   
>
> Vanishing Gradient Problem의 해결책 중 가장 일반적인 해결책은 기존의 `sigmoid`나 `tanh`을 `relu`로 바꾸는 것입니다.  
> 하지만 본문에서 `relu`를 언급하지 않은 이유는, [이전 편](https://gruuuuu.github.io/machine-learning/lstm-doc/#activation-function)에서 언급했듯이 RNN계열은 **같은 레이어를 반복하는 특징**이 있기 때문입니다.  
> RNN계열이 아닌 다른 네트워크 모델들은 relu가 훨---씬 효과적이니 참고하세요!


# LSTM?
## Concept
**Long Short-Term Memory Units**(LSTM)은 기존 RNN의 문제를 극복하기 위해 90년대에 처음 등장한 개념입니다.   

기존 RNN과 마찬가지로 체인구조를 가지고 있지만, 히든레이어에 sigmoid또는 tanh과 같은 활성화함수만 존재했던 RNN과 달리 LSTM은 **4개의 상호작용하는 작은 모듈**들이 존재하고 있습니다.  
<center><img src="https://user-images.githubusercontent.com/15958325/66540998-b3f7d300-eb68-11e9-820e-d49ff8b80ff7.png"></center>   
<center><기존의 RNN 모듈></center>  


<center><img src="https://user-images.githubusercontent.com/15958325/66541012-c114c200-eb68-11e9-84b7-0a1b3b567f48.png"></center> 
<center><4개의 작은 모듈들을 가지고 있는 LSTM></center>  

LSTM을 검색하면 제일 많이 나오는 그림인데, 한눈에 보고 이해하기 굉장히 어렵습니다.  

이 다음 섹션부터는 단계별로 차근차근 설명드리도록 하겠습니다.  

> **LSTM**은 4개의 작은 모듈들을 통해 **장기 의존성 문제**를 해결한다!  

## Deep Dive
