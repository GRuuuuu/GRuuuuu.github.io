---
title: "호다닥 공부해보는 RNN 친구들(2) - LSTM & GRU"
categories: 
  - Machine-Learning
tags:
  - ML
  - RNN
  - LSTM
  - GRU
last_modified_at: 2019-10-05T13:00:00+09:00
author_profile: true
sitemap :
  changefreq : daily
  priority : 1.0
---
[호다닥 공부해보는 RNN 친구들(1) - RNN(Recurrent Neural Networks)](https://gruuuuu.github.io/machine-learning/lstm-doc/)에서 이어지는 글입니다.  

## RNN의 고질적인 문제
이전의 계산결과를 바탕으로 계산한다는 RNN의 특징은 어찌보면 사람의 뇌와 많이 닮았습니다. 그래서 Neural Network라는 이름을 붙인것일지도 모릅니다.  
그래서 사람의 신경망과 닮은 이 특징은 단점도 가지고 있습니다.  

>여러분은 **올해의 신년 다짐**이 뭐였는지 기억나시나요?  
>작년은? 10년전은 기억하고 계신가요?  

물론 어딘가에 기록해두고 꾸준히 실천하고 계신 분들도 있겠지만, 대다수는 그 당시에만 반짝 실천하고 시간이 흐를수록 잊어버리겠죠. 저는 기억도 안납니다.  

RNN도 비슷합니다.  
처음 입력으로 받은 정보는 학습에 강한 영향을 끼치다가 점점 새로운 입력이 들어오면서 처음 입력값의 영향력은 점차 감소하고 결국 학습에 아무런 영향을 끼치지 못하게 됩니다.  

![image](https://user-images.githubusercontent.com/15958325/66256055-1c3f6100-e7c5-11e9-907b-c6182256a32c.png)


이를 RNN의 **장기 의존성 문제**(the problem of Long-Term Dependencies)라고도 하고, 네트워크 학습에 사용하는 경사도(Gradient)가 사라지는(Vanishing)현상이므로 **Vanishing Gradient Problem**이라고도 부릅니다.  

## Vanishing(or Exploding) Gradient Problem
이 문제는 오랜시간 Deep Learning 영역에서 골치를 썩여왔었습니다.   
지금부터는 왜 이런 현상이 발생하는지 알아보도록 하겠습니다.  

먼저 신경망의 역전파에 대해서 알고있어야 하는데 자세한 설명은 다음 동영상으로 대체하고 진행하겠습니다.  :  
[lec9-2: 딥넷트웍 학습 시키기 (backpropagation)](https://youtu.be/573EZkzfnZ0)   

먼저 역전파를 하는 이유는 **결과의 오류를 바로잡기 위함**이고, 이를 위해 각 layer들의 가중치(w)를 조절하게 됩니다.  

cost함수를 최적화하기 위해 기울기를 구하게 되고 이때 기울기는 해당 함수의 미적분값이겠죠.  

문제는 여기서 발생합니다.  
신경망이 **곱하기연산**을 기반으로 만들어져 있기 때문에, 미적값이 1보다 조금만 커도 gradient값이 **발산**하게 되고 1보다 조금만 작아도 gradient값이 **소실**하게 됩니다.  

> EX)  
> 1.1^100 = 13,780.6  
> 0.9^100 = 0.0000265

아래 사진은 sigmoid와 tanh의 그래프와 미적 그래프 입니다.   
![image](https://user-images.githubusercontent.com/15958325/66532733-7042a080-eb4b-11e9-9c0a-23cfd82474fb.png)   
![image](https://user-images.githubusercontent.com/15958325/66532761-910af600-eb4b-11e9-8c67-203f36f022cc.png)   

sigmoid와 tanh 모두 미분함수가 |x|가 커질수록 0에 수렴하는 모양을 보이며 Backpropagation시 미분값이 소실될 가능성이 큽니다.  

그래서 기존 RNN의 **장기 의존성 문제**(the problem of Long-Term Dependencies)가 발생하게 되었고, 이 다음부터 LSTM이 어떻게 문제를 해결하였는지에 대해 설명하도록 하겠습니다.  

> **[리빙포인트 -ReLU]**   
>
> ![image](https://user-images.githubusercontent.com/15958325/66532803-b13ab500-eb4b-11e9-99e2-e01c372b5d93.png)   
>
> Vanishing Gradient Problem의 해결책 중 가장 일반적인 해결책은 기존의 `sigmoid`나 `tanh`을 `relu`로 바꾸는 것입니다.  
> 하지만 본문에서 `relu`를 언급하지 않은 이유는, [이전 편](https://gruuuuu.github.io/machine-learning/lstm-doc/#activation-function)에서 언급했듯이 RNN계열은 **같은 레이어를 반복하는 특징**이 있기 때문입니다.  
> RNN계열이 아닌 다른 네트워크 모델들은 relu가 훨---씬 효과적이니 참고하세요!


# LSTM?
## Concept
**Long Short-Term Memory Units**(LSTM)은 기존 RNN의 문제를 극복하기 위해 90년대에 처음 등장한 개념입니다.   

기존 RNN과 마찬가지로 체인구조를 가지고 있지만, 히든레이어에 sigmoid또는 tanh과 같은 활성화함수만 존재했던 RNN과 달리 LSTM은 **4개의 상호작용하는 작은 모듈**들이 존재하고 있습니다.  
<center><img src="https://user-images.githubusercontent.com/15958325/66540998-b3f7d300-eb68-11e9-820e-d49ff8b80ff7.png"></center>   
<center><기존의 RNN 모듈></center>  


<center><img src="https://user-images.githubusercontent.com/15958325/66541012-c114c200-eb68-11e9-84b7-0a1b3b567f48.png"></center> 
<center><4개의 작은 모듈들을 가지고 있는 LSTM></center>   



LSTM 레이어를 하나 떼어서 라벨을 붙이면 다음과 같은 그림이 나오게 됩니다.  
<center><img src="https://user-images.githubusercontent.com/15958325/66697080-5cdf3300-ed0d-11e9-9264-93917ca8e453.png"></center>    

한 눈에 보이는 LSTM의 특징은 다음과 같습니다.  
1. 레이어 안에는 **4개의 모듈** 존재
2. 레이어 밖으로 나가는 선이 **두개**

### 1.Gates
앞서 언급한 4개의 모듈은 3개의 Gate로 표현됩니다.  

- **forget gate (f)** : **과거 정보를 잊기**위한 게이트. 시그모이드 함수의 출력 범위는 0 ~ 1 이기 때문에 그 값이 0이라면 이전 상태의 정보는 잊고, 1이라면 이전 상태의 정보를 온전히 기억하게 된다.  
- **input gate (i)** : **현재 정보를 기억하기**위한 게이트다. 
- **output gate (o)** : 최종 결과 **h**를 위한 게이트


### 2.Cell State

![image](https://user-images.githubusercontent.com/15958325/66697370-eee83b00-ed0f-11e9-8f5d-cb8183b2f581.png)  

먼저 맨 위를 가로지르는 선 C를 **cell state**라고 부릅니다.  

가장 눈에 띄는 부분은 **더하기(+)기호**입니다. 일반적인 RNN에서는 곱하기 연산으로만 이루어져 있었는데 LSTM에서는 피드백을 더하기(+)로 이음으로써 **Vanishing Gradient**와 같은 문제를 해결할 수 있습니다.  

cell state 또한 이전에 배운 은닉 상태처럼 이전 시점의 cell state가 다음 시점의 cell state를 구하기 위한 입력으로서 사용됩니다.

> **LSTM**은 이 cell state를 보호하고 컨트롤 하기 위한 세 가지 게이트: forget, input, output gate를 통해 vanishing gradient를 방지하고 그래디언트가 효과적으로 흐를 수 있게 한다.

## Deep Dive


