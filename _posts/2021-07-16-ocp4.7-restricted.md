---
title: "Openshift4.7 Baremetal 설치 - Restricted Network"
categories: 
  - OCP
tags:
  - Kubernetes
  - RHCOS
  - VMware
  - Openshift
last_modified_at: 2021-07-16T13:00:00+09:00
toc : true
author_profile: true
sitemap :
  changefreq : daily
  priority : 1.0
---

## Overview
작년에 Openshift 4.3에대한 baremetal UPI방식, 그리고 폐쇄망에서의 설치에 대해서 기술한 적이 있습니다.  
- [Openshift4.3 Installation on Baremetal](https://gruuuuu.github.io/ocp/ocp4-install-baremetal/)  
- [Openshift4.3 Baremetal 설치 - Restricted Network](https://gruuuuu.github.io/ocp/ocp-restricted-network/)  

시간이 지나면서 설치방식에 조금씩 변화가 생겼고, 원래 문서에 업데이트를 하려다가 하나의 문서에 여러 버전을 적는것도 별로 좋지 않은 것 같아서 새로 문서를 작성하려고 합니다.  

그래서 이번 문서에서는 위 두 포스팅 대비 바뀐 점들 위주로 기술하려고 합니다.  
설치에 대한 자세한 설명은 위 링크로 대신하겠습니다.  

RH공식 문서 -> [Installing a cluster on bare metal in a restricted network](https://docs.openshift.com/container-platform/4.7/installing/installing_bare_metal/installing-restricted-networks-bare-metal.html)  


## Prerequisites

사용한 머신의 스펙:  

|name|os|cpu|ram|stg|role|
|---|--|--|--|--|--|
Infra1|CentOS 7|4|8G|60GB|dns, LB|
Infra2|CentOS 8|4|8G|1TB|Bastion, Mirror Regsitry, FileServer|
Bootstrap|CoreOS 4.7|4|16G|100GB|Bootstrap|
Master0|CoreOS 4.7|6|20G|300GB|Master|
Master1|CoreOS 4.7|6|20G|300GB|Master|
Master2|CoreOS 4.7|6|20G|300GB|Master|

3Node(Master가 Worker역할도 함)구성으로 진행

>**Tip)**  
>Openshfit 4.7에서 사용하는 몇몇 커맨드와 dependency(ex. `opm`, `glibc` 등)가 버전이 꽤 높기 때문에 CentOS7에서 기본적으로 제공하는 패키지로는 설치하기 어려운 경우가 많습니다.  
>예를들어, `podman`같은 경우 권장사항은 v1.9.3 이상인데, CentOS7 epel 기본 버전은 1.6입니다.  
>1.9이상을 깔기 위해 추가적인 바이너리 설치가 요구됩니다.  
>**되도록이면 CentOS 8을 사용하도록 합시다!**  


## 1. Infra1 - DNS, LoadBalancer 구성
### 1.1 DNS
[DNS설정 -v4.3](https://gruuuuu.github.io/ocp/ocp-restricted-network/#dns)  
위 포스팅과 동일한 과정을 진행합니다.  

v4.5부터 etcd와 srv레코드가 없어졌는데 참고하여 DNS 레코드를 다시 구성해주었습니다.  
### 정방향 DNS
~~~
$TTL 1D
@   IN SOA  @ hololy.net. (
                    20210713 ; serial
                    1D  ; refresh
                    1H  ; retry
                    1W  ; expire
                    3H )    ; minimum
    IN NS   hololy.net.
    IN A    10.178.105.2
ns  IN A    10.178.105.2
www IN A    10.178.105.2

;cluster name
test   IN CNAME    @

api-int.test   IN A 10.178.105.2
api.test   IN A 10.178.105.2
*.apps.test    IN A 10.178.105.2

;ocp cluster
bootstrap.test   IN  A   10.178.105.4
master0.test     IN  A   10.178.105.5
master1.test     IN  A   10.178.105.6
master2.test     IN  A   10.178.105.7

;ocp registry (only for restrict network)
registry.test        IN  A  10.178.105.3
~~~

역방향 DNS는 예전 포스팅과 동일하게 진행해줍니다.  

### 1.2 LoadBalancer
[이전 포스팅](https://gruuuuu.github.io/ocp/ocp-restricted-network/#loadbalancer)에서는 nginx의 LoadBalancing기능을 사용했었는데요  

대중적으로 많이 사용하는 로드밸런서에는 `haproxy`가 있는데 이번엔 요놈을 써서 로드밸런서를 구성해보려고 합니다.  

nginx와 성능은 크게 차이나지않는다고 하지만 haproxy는 **로드밸런싱할 노드들의 health check를 해준다는 점**이 가장 큰 차이점입니다.(nginx는 pro에서만 해당 기능 제공)  

>사실 엔터프라이즈 환경에서는 SW 로드밸런서보다 L4스위치와 같은 피지컬 장비를 쓰는 곳이 많기때문에 테스팅환경에서는 어떤 로드밸런서를 사용하셔도 무관합니다.  

패키지로 설치하셔도 좋고, 저는 최신 haproxy를 써보고 싶어서 바이너리 빌드하는 방식으로 설치하였습니다.  

### haproxy 설치

필요 패키지 설치:  
~~~sh
$ yum install gcc openssl openssl-devel pcre-static pcre-devel systemd-devel wget
~~~

download -> [http://www.haproxy.org/](http://www.haproxy.org/)  

~~~sh
$ wget http://www.haproxy.org/download/2.4/src/haproxy-2.4.2.tar.gz
$ tar xvp haproxy-2.4.2.tar.gz
~~~

설치 옵션 설정:  
~~~sh
$ make TARGET=linux-glibc USE_OPENSSL=1 USE_PCRE=1 USE_ZLIB=1 USE_SYSTEMD=1

  CC      src/ssl_sample.o
  CC      src/ssl_sock.o
  CC      src/ssl_crtlist.o
  CC      src/ssl_ckch.o
  CC      src/ssl_utils.o
  CC      src/cfgparse-ssl.o
  CC      src/namespace.o
  CC      src/mux_h2.o
  CC      src/mux_fcgi.o
...
~~~

설치:  
~~~sh
$ make install

‘haproxy’ -> ‘/usr/local/sbin/haproxy’
‘doc/haproxy.1’ -> ‘/usr/local/share/man/man1/haproxy.1’
install: creating directory ‘/usr/local/doc’
install: creating directory ‘/usr/local/doc/haproxy’
‘doc/configuration.txt’ -> ‘/usr/local/doc/haproxy/configuration.txt’
…
~~~

설치 확인:  
~~~sh
$ haproxy -v

HAProxy version 2.4.2-553dee3 2021/07/07 - https://haproxy.org/
Status: long-term supported branch - will stop receiving fixes around Q2 2026.
Known bugs: http://www.haproxy.org/bugs/bugs-2.4.2.html
Running on: Linux 3.10.0-1160.el7.x86_64 #1 SMP Mon Oct 19 16:18:59 UTC 2020 x86_64
~~~

systemd 서비스로 등록:  
~~~sh
$ wget https://raw.githubusercontent.com/haproxy/haproxy/master/admin/systemd/haproxy.service.in -O /etc/systemd/system/haproxy.service
~~~

서비스 등록 후, haproxy의 sbin 경로 재설정 (" @SBINDIR@ 를 /usr/local/sbin 으로 변경 ")  
~~~sh
ExecStartPre=@SBINDIR@/haproxy -f $CONFIG -c -q $EXTRAOPTS
ExecStart=@SBINDIR@/haproxy -Ws -f $CONFIG -p $PIDFILE $EXTRAOPTS
ExecReload=@SBINDIR@/haproxy -f $CONFIG -c -q $EXTRAOPTS
==>
ExecStartPre=/usr/local/sbin/haproxy -f $CONFIG -c -q $EXTRAOPTS
ExecStart=/usr/local/sbin/haproxy -Ws -f $CONFIG -p $PIDFILE $EXTRAOPTS
ExecReload=/usr/local/sbin/haproxy -f $CONFIG -c -q $EXTRAOPTS
~~~

config파일 생성

~~~conf
$ vim /etc/haproxy/haproxy.cfg


global
    daemon
    # transmit UDP to rsyslog
    log 127.0.0.1 local0

defaults
    log               global
    timeout connect   5s
    timeout client    50s
    timeout server    50s

frontend kubernetes_api
    bind 0.0.0.0:6443
    default_backend kubernetes_api
    option tcplog

backend kubernetes_api
    balance roundrobin
    server bootstrap bootstrap.test.hololy.net:6443 check
    server master0 master0.test.hololy.net:6443 check
    server master1 master1.test.hololy.net:6443 check
    server master2 master2.test.hololy.net:6443 check

frontend machine_config
    bind 0.0.0.0:22623
    default_backend machine_config
    option tcplog

backend machine_config
    balance roundrobin
    server bootstrap bootstrap.test.hololy.net:22623 check
    server master0 master0.test.hololy.net:22623 check
    server master1 master1.test.hololy.net:22623 check
    server master2 master2.test.hololy.net:22623 check

frontend router_https
    bind 0.0.0.0:443
    default_backend router_https

backend router_https
    balance roundrobin
    server master0 master0.test.hololy.net:443 check
    server master1 master1.test.hololy.net:443 check
    server master2 master2.test.hololy.net:443 check

frontend router_http
    mode http
    bind 0.0.0.0:80
    default_backend router_http

backend router_http
    mode http
    balance roundrobin
    server master0 master0.test.hololy.net:80 check
    server master1 master1.test.hololy.net:80 check
    server master2 master2.test.hololy.net:80 check

frontend ocp_registry
    bind 0.0.0.0:5000
    default_backend ocp_registry
    option tcplog

backend ocp_registry
    server registry 10.178.105.3:5000 check

~~~

haproxy관련 directory 생성  
~~~sh
$ sudo mkdir -p /etc/haproxy 
$ sudo mkdir -p /var/log/haproxy 
$ sudo mkdir -p /etc/haproxy/certs 
$ sudo mkdir -p /etc/haproxy/errors/
~~~

config파일이 제대로 되었는지 확인  
~~~sh
$ haproxy -f /etc/haproxy/haproxy.cfg -c
Configuration file is valid
~~~

haproxy서비스 실행  
~~~sh
$ systemctl enable haproxy
$ systemctl start haproxy 
~~~

그런데 돌아가긴하는데 warning이 뜹니다. 아직 각 노드가 뜨지 않았기 때문입니다. 설치 후에 보면 warning이 없어져있어서 괜찮습니다.  
~~~sh
$ systemctl status haproxy

● haproxy.service - HAProxy Load Balancer
   Loaded: loaded (/etc/systemd/system/haproxy.service; disabled; vendor preset: disabled)
   Active: active (running) since Wed 2021-07-14 00:57:20 EDT; 3s ago
  Process: 2867 ExecStartPre=/usr/local/sbin/haproxy -f $CONFIG -c -q $EXTRAOPTS (code=exited, status=0/SUCCESS)
 Main PID: 2870 (haproxy)
   CGroup: /system.slice/haproxy.service
           ├─2870 /usr/local/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock
           └─2874 /usr/local/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock

Jul 14 00:57:22 93.47.38a9.ip4.static.sl-reverse.com haproxy[2870]: [WARNING]  (2874) : Server machine_config/bootstrap is DOWN, reason: Layer4 time
Jul 14 00:57:22 93.47.38a9.ip4.static.sl-reverse.com haproxy[2870]: [WARNING]  (2874) : Server machine_config/master0 is DOWN, reason: Layer4 timeou
Jul 14 00:57:23 93.47.38a9.ip4.static.sl-reverse.com haproxy[2870]: [WARNING]  (2874) : Server machine_config/master1 is DOWN, reason: Layer4 timeou
Jul 14 00:57:23 93.47.38a9.ip4.static.sl-reverse.com haproxy[2870]: [ALERT]    (2874) : backend 'machine_config' has no server available!
Jul 14 00:57:23 93.47.38a9.ip4.static.sl-reverse.com haproxy[2870]: [WARNING]  (2874) : Server router_https/master0 is DOWN, reason: Layer4 timeout
~~~

### [Optional] haproxy log설정
`haproxy`서비스를 띄웠으니, 이제 `rsyslog`에 haproxy용 로그를 설정해줄겁니다.  
haproxy.cfg에서 아래와 같이 global 섹션을 정의해주었는데,  
~~~
global
    daemon
    # transmit UDP to rsyslog
    log 127.0.0.1 local0
~~~
이는 haproxy에서 수집된 로그들을 rsyslog에 UDP로 전송한다는 의미입니다.  

그래서 이젠 `rsyslog` 어떻게 haproxy로그들을 처리할건지 정의해주면 됩니다.  
~~~sh
$ vim /etc/rsyslog.d/haproxy.conf

# Provides UDP syslog reception
$ModLoad imudp
$UDPServerRun 514
$template Haproxy, "%msg%\n"
#rsyslog 에는 rsyslog 가 메세지를 수신한 시각 및 데몬 이름같은 추가적인 정보가 prepend 되므로, message 만 출력하는 템플릿 지정
# 이를 haproxy-info.log 에만 적용한다.

# 모든 haproxy 를 남기려면 다음을 주석해재, 단 access log 가 기록되므로, 양이 많다.
#local0.*   /var/log/haproxy/haproxy.log

# local0.=info 는 haproxy 에서 에러로 처리된 이벤트들만 기록하게 됨 (포맷 적용)
local0.=info    /var/log/haproxy/haproxy-info.log;Haproxy

# local0.notice 는 haproxy 가 재시작되는 경우와 같은 시스템 메세지를 기록하게됨 (포맷 미적용)
local0.notice   /var/log/haproxy/haproxy-allbutinfo.log
~~~
-> 참고한 포스팅: [Haproxy 설치해서 로드 밸런서로 활용하기](https://findstar.pe.kr/2018/07/27/install-haproxy/)  


이제 로그설정을 마쳤으니, 추가적으로 로그의 회전주기를 관리해주어야 합니다. 안해주면 로그가 끝없이 쌓여서 용량을 잡아먹게되니까요!  
짧은기간 테스트할거면 상관없지만 긴 기간동안 클러스터를 운영할 것이라면 필수적으로 해주어야 합니다.  
`logrotate`는 로그의 회전 주기를 관리하는 서비스 입니다.  
~~~sh
$ vim /etc/logrotate.d/haproxy

/var/log/haproxy/*log {
    daily
    rotate 90
    create 0644 nobody nobody
   dateext
    missingok
    notifempty
    compress
    sharedscripts
    postrotate
        /bin/systemctl restart rsyslog.service > /dev/null 2>/dev/null || true
    endscript
}
~~~

- rotate 90 (90-daily ->90일에 한번씩 회전)
- create 0644 nobody nobody    -> 로그파일 정리 후 새로운 로그파일 생성
- dateext -> logrotate실행 뒤 로그파일에 날짜 부여
- missingok : 로그파일이 없을경우 에러메시지를 출력하고 다음으로 실행합니다
- notifempty : 로그파일의 내용이 없을경우 rotate 하지 않습니다
- compress : 로그파일을 압축합니다
- sharedscripts : 여러개의 로그파일을 스크립트로 공유하여 실행합니다
- postrotate : 실행 후 스크립트 파일 실행합니다(rsyslog재시작)  

rsyslog 재시작:  
~~~sh
$ systemctl restart rsyslog
~~~

LISTEN하고 있는 port를 살펴보면 haproxy에 설정했던 포트들이 정상적으로 listen중이라는 것을 확인할 수 있습니다.  
~~~sh
$ netstat -tnlp

Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN      24201/named
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1790/master
tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      6426/haproxy
tcp        0      0 0.0.0.0:22623           0.0.0.0:*               LISTEN      6426/haproxy
tcp        0      0 0.0.0.0:6443            0.0.0.0:*               LISTEN      6426/haproxy
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      6426/haproxy
...
~~~

### 1.3 Security - Firewall, SElinux

기본적으로 `SElinux`는 `enforcing` 상태입니다.  
이걸 `Permissive`상태로 변환하여 모든 요청에 대해 warning만 날리게 할 수도 있지만,  
보안적으로는 굉장히 좋지 않은 선택입니다.  

그래서 이번 테스트에서는 방화벽과 selinux 모두 끄지 않고 진행할 예정입니다.  

### SElinux 설정

상태확인:  
~~~sh
$ sestatus
SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             targeted
Current mode:                   enforcing
Mode from config file:          enforcing
Policy MLS status:              enabled
Policy deny_unknown status:     allowed
Max kernel policy version:      31
~~~

현재 selinux에서 허용하고 있는 포트 확인  
~~~sh
$ semanage port -l |grep http_port_t
http_port_t                    tcp      80, 81, 443, 488, 8008, 8009, 8443, 9000
pegasus_http_port_t            tcp      5988
~~~

LoadBalancer에서 사용하고 있는 6443, 22623, 5000 포트를 추가  
~~~sh
$ semanage port -a -t http_port_t -p tcp 22623
$ semanage port -a -t http_port_t -p tcp 6443
$ semanage port -a -t http_port_t -p tcp 5000
~~~

### 방화벽 설정
현재 허용하고 있는 port list:   
~~~sh
$ firewall-cmd --zone=public --list-ports
$ firewall-cmd --zone=internal --list-ports
~~~

규칙 추가:  
~~~sh
# http
firewall-cmd --add-port=80/tcp --zone=internal --permanent
firewall-cmd --add-port=80/tcp --zone=public --permanent

# https
firewall-cmd --add-port=443/tcp --zone=internal --permanent
firewall-cmd --add-port=443/tcp --zone=public --permanent

# kubernetes api
firewall-cmd --add-port=6443/tcp --zone=internal --permanent
firewall-cmd --add-port=6443/tcp --zone=public --permanent

# Machine config server
firewall-cmd --add-port=22623/tcp --zone=internal --permanent
firewall-cmd --add-port=22623/tcp --zone=public --permanent

# mirror registry
firewall-cmd --add-port=5000/tcp --zone=internal --permanent
firewall-cmd --add-port=5000/tcp --zone=public --permanent

# dns
firewall-cmd --add-port=53/tcp --zone=internal --permanent
firewall-cmd --add-port=53/tcp --zone=public --permanent
firewall-cmd --add-port=53/udp --zone=internal --permanent
firewall-cmd --add-port=53/udp --zone=public --permanent
~~~

방화벽 reload:  
~~~sh
$ firewall-cmd --reload 
~~~

## 2. Infra2 - Bastion, Mirror Regsitry, FileServer 구성

### 2.1 Bastion
[기존](https://gruuuuu.github.io/ocp/ocp-restricted-network/#1-installer-download)과 동일.  

사용한 command 버전 정보:  
~~~sh
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.1-5-g76a04fc", GitCommit:"8b4b09487463415374368af3bbc4ff2e6366477b", GitTreeState:"clean", BuildDate:"2021-07-07T06:08:23Z", GoVersion:"go1.15.7", Compiler:"gc", Platform:"linux/amd64"}

$ oc version
Client Version: 4.7.20
~~~

mirror registry를 다루기위해 `opm`도 설치해줍니다.(동일한 [링크](https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.7.20/)에 `opm-linux-4.7.20.tar.gz`를 다운로드 받고 압축해제)
~~~sh
$ opm version
Version: version.Version{OpmVersion:"v1.15.4-16-g06e950de", GitCommit:"06e950de5ebca66e493f6cd2414e73c8978090d3", BuildDate:"2021-07-07T06:32:23Z", GoOs:"linux", GoArch:"amd64"}
~~~

### 2.2 Security - Firewalld, SElinux
Infra1과 동일하게 사용할 포트에 대한 규칙을 만들어 줍시다.  
#### Firewalld
~~~sh
#mirror-registry
firewall-cmd --add-port=5000/tcp --zone=internal --permanent
firewall-cmd --add-port=5000/tcp --zone=public --permanent
# fileserver
firewall-cmd --add-port=8080/tcp --zone=internal --permanent
firewall-cmd --add-port=8080/tcp --zone=public --permanent

firewall-cmd --reload 
~~~

#### SElinux
~~~sh
$ semanage port -a -t http_port_t -p tcp 5000
$ semanage port -a -t http_port_t -p tcp 8080
~~~

### 2.3 Mirror Registry 구성

[여기](https://gruuuuu.github.io/ocp/ocp-restricted-network/#mirrorregistry)서 4.레지스트리 사용자 생성까지 진행.  

>주의) podman v1.9.3+ 
>~~~sh
>$ podman version
>Version:      3.0.2-dev
>API Version:  3.0.0
>Go Version:   go1.15.7
>Built:        Fri Jun 11 11:58:44 2021
>OS/Arch:      linux/amd64
>
>$ grpcurl -version
>grpcurl v1.8.1
>~~~

registry 컨테이너 띄우기:  
~~~sh
$ podman run --name mirror-registry -p 5000:5000 \
  -v /opt/registry/data:/var/lib/registry:z \
  -v /opt/registry/auth:/auth:z \
  -e "REGISTRY_AUTH=htpasswd" \
  -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
  -v /opt/registry/certs:/certs:z \
  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \
  -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \
  -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true \
  -d docker.io/library/registry:2
~~~

확인:  
~~~sh
$ curl -u admin:passw0rd -k https://registry.test.hololy.net:5000/v2/_catalog

{"repositories":[]}
~~~

로그인하기:  
~~~sh
$ podman login -u admin -p passw0rd registry.test.hololy.net:5000

Login Succeeded!
~~~

pull secret 설정: [여기](https://cloud.redhat.com/openshift/install/pull-secret)서 pull secret 복사하여 `pull-secret.json`파일로 생성. 여기에 로컬 registry의 auth 정보도 추가해줍니다.    
==> [참고](https://gruuuuu.github.io/ocp/ocp-restricted-network/#7-pull-secret-%EC%84%A4%EC%A0%95)  

다만들고 옮김:  
~~~sh
$ mv pullsecret.json /opt/registry/certs/pull-secret.json
~~~

[파라미터](https://gruuuuu.github.io/ocp/ocp-restricted-network/#8-mirroring) 참고하여 Mirroring 시작  
~~~sh
$ oc adm -a ${LOCAL_SECRET_JSON} release mirror \
   --from=quay.io/${PRODUCT_REPO}/${RELEASE_NAME}:${OCP_RELEASE} \
   --to=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} \
   --to-release-image=${LOCAL_REGISTRY}/${LOCAL_REPOSITORY}:${OCP_RELEASE}
~~~
![image](https://user-images.githubusercontent.com/15958325/126042931-723c8e8c-4af0-4202-a6d3-c3f9b0ea1bd9.png)  


다끝나면 다음과 같이 registry에 목록을 확인할 수 있습니다.  
~~~sh
$ curl -u admin:passw0rd -k https://registry.test.hololy.net:5000/v2/_catalog                                                                                   {"repositories":["ocp4.7.20-x86_64"]}
~~~

### 2.4 File Server 구성
[과정 동일](https://gruuuuu.github.io/ocp/ocp-restricted-network/#http-file-server)  

## 3. Installation
### 3.1 설치준비
이 [문서](https://gruuuuu.github.io/ocp/ocp-restricted-network/#installation)참조.  
CoreOS설치까지 동일!  

>주의)  
>Worker노드가 없는 3노드 구성을 하려면, Manifest파일을 생성하고 `manifest/cluster-scheduler-02-config.yml`의 `mastersSchedulable`파라미터가 **true**로 되어있는지 확인하셔야 합니다.  
>~~~yaml
>apiVersion: config.openshift.io/v1
>kind: Scheduler
>metadata:
>  creationTimestamp: null
>  name: cluster
>spec:
>  mastersSchedulable: True
>  policy:
>    name: ""
>status: {}
>~~~

### 3.2 CoreOS 설치
이제 4.7로 올라오면서 베어메탈 UPI방식의 혁명적인 변경점에 대해서 말씀드리겠습니다.(사실 4.6부터 바뀜)  

예전에는 OS 커널 파라미터를 일일히 쳐주어야 했습니다.  
~~~sh
coreos.inst.install_dev=sda 
coreos.inst.image_url=http://{fileserver}:8080/{coreos raw file}.raw.gz
coreos.inst.ignition_url=http://{fileserver}:8080/{ignition file}.ign 
ip={static ip}::{gateway}:{netmask}:{hostname}:{network interface}:none
nameserver={dns}
~~~
설치 시작 전에 initial 파라미터를 넣어주느라 오타라도 나면 다시 설치 첫번째 페이지로 돌아와서 입력해야만 했습니다.  

하지만 이제는 CoreOS에 `coreos-installer`커맨드가 내장되어, 임시 OS를 부트시킨 뒤 파라미터를 입력시켜줄 수 있게 되었습니다.  

>raw파일 다운받을 필요 없음!

iso 파일을 부팅시키고 나면 다음과 같이 초기 CoreOS의 shell을 확인할 수 있습니다.  
![image](https://user-images.githubusercontent.com/15958325/126058140-55fe40f1-1835-4cfa-ae7e-dd0e1760bc56.png)  

nmcli나 nmtui로 네트워크 설정:  

**nmcli:**  
~~~sh
$ nmcli con add con-name {connection name} ifname {nic} autoconnect {boot시 연결 yes||no} ip4 {ipv4 address} gw4 {gateway} ipv4.dns {DNS (구분자는 공백"8.8.8.8 8.8.4.4")} ipv4.method {auto||manual -> dhcp냐 static이냐}

==>

$ nmcli con add con-name static ifname ens192 autoconnect yes type ethernet ip4 10.178.105.4/26 gw4 10.178.105.2 ipv4.dns 10.178.105.2 ipv4.method manual
~~~

**nmtui:**  
~~~sh
$ nmtui
~~~
![image](https://user-images.githubusercontent.com/15958325/126058188-783e717e-36d9-4356-a8f1-599519597767.png)  

이렇게 설정한 네트워크 정보들은 `/etc/NetworkManager/system-connections/static.nmconnection`에 저장되게 됩니다.  

![image](https://user-images.githubusercontent.com/15958325/126058203-795119cb-6de6-4de6-b974-a38462cbea70.png)  

>**주의)** 이상태로 reboot하게되면 설정했던 네트워크 정보들은 날아가게 됩니다.  
>coreos-installer 전까지는 iso 부팅해도 tmpfs 로 된 ramdisk 만 사용하고, 모든 변경은 디스크에 저장되지 않기 때문입니다.  

이제 network설치를 마쳤으니 ignition파일을 설치해주도록 하겠습니다.  
~~~sh
$ sudo coreos-installer install --copy-network --ignition-url=http://{FILE_SERVER}:8080/bootstrap.ign --insecure-ignition /dev/sda
~~~

- `--copy-network` : host의 network 설정을 카피(`/etc/NetworkManager/system-connections/static.nmconnection` 여기있는거만 카피)
- `--insecure-ignition` : http로 다운받으려면 필수 파라미터

![image](https://user-images.githubusercontent.com/15958325/126058306-ab70ab2c-6b76-4749-b18b-8d87b56c7d39.png)  

파라미터를 틀려도... 바로 다시 칠 수 있고.. 아주 좋은 패치입니다ㅎㅎ

ignition설치가 성공적으로 마무리 되었다면 reboot를 해서 ignition파일을 적용시켜주도록 합니다.  
~~~sh
$ reboot
~~~

![image](https://user-images.githubusercontent.com/15958325/126058333-ac4f1be4-15d6-455f-b9e3-ae3c9ee65630.png)  

다른 노드들도 똑같이! 해서 설치를 마무리 지어줍시다.  
![image](https://user-images.githubusercontent.com/15958325/126058356-b6595d2c-8d66-46c8-baab-748948f066d5.png)  

끝!  

-> [설치 Troubleshooting](https://gruuuuu.github.io/ocp/ocp4-install-error/)  
-> [Operator Hub 구축하기](https://gruuuuu.github.io/ocp/operatorhub/)  

----
